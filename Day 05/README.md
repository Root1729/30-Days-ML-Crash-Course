# 📘 Day 05: Math for Machine Learning

## 🧠 Objective
Develop a solid understanding of the **mathematics that underpin machine learning algorithms**. By the end of the day, you’ll be able to confidently understand the concepts that guide models behind the scenes.

---

## 📌 Topics Covered

### 📐 Linear Algebra
- Vectors, matrices, dot product, matrix multiplication
- Transpose, identity matrix
- Matrix-vector multiplication in ML (e.g., X·W + b)

### 🔁 Calculus
- Derivatives & partial derivatives
- Gradients & gradient descent
- Visualization of a loss surface and gradient steps

### 🎲 Probability & Statistics
- Mean, variance, standard deviation
- Bayes' Theorem, probability distributions
- Normal distribution, Bernoulli, binomial (introduction)

### ⚙️ Optimization
- Cost function
- Gradient descent: concept and visualization

---

## 💻 Hands-On Practice
- [ ] Visualize vectors and matrix multiplication using NumPy
- [ ] Simulate normal distributions and compute stats with Python
- [ ] Animate gradient descent steps for a quadratic function
- [ ] Manually compute gradients and compare with `sympy` or `autograd`

---

## 🔗 Resources
- [Essence of Linear Algebra (3Blue1Brown)](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)
- [Khan Academy - Intro to Calculus](https://www.khanacademy.org/math/calculus-1)
- [StatQuest - ML Math](https://www.youtube.com/c/joshstarmer)
- [Python NumPy Guide](https://numpy.org/doc/)

---

## ✅ Challenge
- Derive the gradient of the Mean Squared Error loss
- Simulate a dataset and apply linear regression from scratch using math
- Visualize the cost function for a regression problem

---

> ✨ A strong math foundation helps you go beyond using ML — it helps you **understand it**.
